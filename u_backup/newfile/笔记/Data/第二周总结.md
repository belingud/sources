# 机器学习原理

1. 学习器（KNN、LinearRegression、Ridge、Lasso、LosgistidRegression、DecisionTree）

2. 构造损失函数（）

误差小：

- 准确率：分类模型
- 均方误差：线性模型

目的是求最小误差

方法：

- 梯度下降
- 最大似然估计

构造：

- 最小二乘：线性回归模型

3. 估计函数（得到了解的方程）

# 模型

### KNN（回归分类）

**算法原理：** 欧氏距离（数据样本必须是可运算的）

数据样本映射的方法：

- 有序：可以进行比较的数据
- 无序：不能比较的数据，比如男女[0, 1]==>男, [1, 0]==>女
- 使用字典映射：
    - 需要对数据做整合（公务员、国家公务员==公务员）
    
    ```python
				    map_dic = {公务员: 公务员, 国家公务员: 公务员}
    f = lambda x: map_dic.get(x, x)
    df['职位'].map()
    ```
    - 对有序数据进行映射：搞清数据的大小关系
        - 影响算法的先后顺序：模型、算法、算法参数
        - 超参：alpha就是超参，一般用于调整算法的偏差，缩减方差
        - 算法参数：算法经过调整之后，得到的算法的本身系数
        - 原则：通过超参的调整来影响算法参数，来达到预测准确的目的
		
优点：准确度高，异常值不敏感

缺点：预测时间长，对大数据集处理效果不好


## LinearRegression

基本原理，用最小二乘法作为损失函数，用梯度下降来找到极值点

学习模型：$f(x) = w \times x + b$

损失函数：最小二乘法

求解最小值：梯度下降发($w/b$求偏导)

解决一般问题主要用如下两种模型：

1. Ridge（L2范数：正则项$|{\sum _{i=1} ^N|x_i|}|$）    引入$\lambda$系数，对参数进行微调，往0方向压缩
2. Lasso（L1范数：正则项$|\sqrt {{\sum _{i=1} ^N{x_i}^2}}|$）    引入$\lambda$系数，对参数进行微调，向0压缩

两种算法都属于缩减算法。意义在于可以更好的理解数据，有效剔除噪声，避免过拟合


## LogisticRegression

广义线性回归模型

学习模型：$sigmod = {{1}\over{1+e^{-x}}}$和$f(x) = w \times x + b$

通过求解一个样本是正样本的概率


## 决策树

ID3算法、C4.5、CART（二叉树）

ID3算法，就是一种贪心算法，用来计算信息熵，计算在每一个特征上的信息增益，用信息增益来获得决策路径。

可以有效的观察到决策树的决策过程

绘制决策树（利用graphviz）：

```python
dot = tree.export_graphviz()
g = graphviz.Source(dot)
g
```

构造决策树：

1. 特征是离散值，不要求生成二叉树，直接利用每一个属性作为分支
2. 特征是离散值，要求生成二叉树，分成两个自己，属于此子集作为一个分支，不属于作为另一个分支
3. 特征是连续值，确定一个分裂点：split_point。小于这个点分为一个类，大于分裂点分为另一个类

决策树并不常用，应用较多的是随机森林、极度随机森林
