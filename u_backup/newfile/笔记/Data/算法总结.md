# KNN算法

KNN算法又叫临近算法，或者K最近邻分类算法，是数据挖掘分类计数中，最简单的方法之一。所谓kK最近邻，就是K个最近邻的意思，说的是每个样本都可以用他最接近的K个邻居来代表。

KNN算法的核心思想是，如果一个样本在特征空间中的K个最近邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最近邻的一个或者几个样本的类别来决定待分类样本所属的类别。KNN算法在类别决策时，只与极少量的相邻样本有关。由于KNN算法主要靠周围有限的临近的样本，而不是靠判别类域的方法来确定所属类别，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。

KNN是非参数的，基于实例的算法。非参数意味着其不再底层的数据分布上进行任何的臆测，而基于实例意味着不是明确地学习一个模型，而是选择记忆训练的实例们。

## KNN算法的几个概念

**相似度：欧氏距离**

KNN的名字叫K近邻，如何叫近，需要一个数学上的定义，最常用的是用欧氏距离，即N维空间中两点之间的距离。

欧式距离：$$d =\sqrt{\sum_{i=1}^N{ (x_{1i} - x_{2i})}}$$

其他常用距离方法：

- 汉明距离（Hamming Distance）
- 曼哈顿距离 (Manhattan Distance）
- 闵可夫斯基距离（Minkowski Distance）

**K取值**

K取值越小，意味着数据噪音会在结果上有很大的影响。K取值越大，会增大计算成本。K很大程度上以来与经验，由自己衡量。

**惰性学习**

KNN算法不像决策树、朴素贝叶斯这些急切学习法，惰性学习只是简单的存储训练元组，做一些少量工作，真正进行分类或预测的时候才开始做更多的工作。

## KNN算法的优缺点

优点：精度奇高，对一场数据不敏感，非常流行

缺点：不适合大数据集和该维度的特征空间的运算

特点：学习快，预测慢，并没有推导出一个估计函数

调参：n_neighbors来调整拟合度，经验值：不大于样本个数的平方根

## KNN算法的步骤

1. 首先是获取数据，很多时候进行数据分析，需要读取数据表、数据集来进行分析，读取数据集用到`pandas`中的几个方法.

- read_csv()：可以读取csv、tsv、txt等的数据
    - `sep='\t'`：分隔符，默认为空
    - `header=0`：列标签的行，默认第一行
- read_excel()：读取excel数据

2. 创建学习器对象

```python
from sklearn.neighbors import KNeighborsClassifier
# 创建学习对象
knnclf = KNeighborsClassifier()
```

获取数据的有价值信息，将其分为训练数据和目标数据，训练数据是特征数据，而目标数据是需要得到的分类或者预测。

```python
train = seed_raw[[0, 1, 2, 3, 4, 5, 6]]
target = seed_raw[7]
```


学习期对象会记录下数据，但是不会直接得出模型结论，当需要使用这个模型获得结果时才会进行计算。

```python
from sklearn.model_selection import train_test_split
# 对样本进行拆分，目的是通过算法评价指标，找到最优算法
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2)
```

3. 进行训练和评分

训练模型使用`model.fit(X_train, y_train)`方法

预测数据使用`model.predict(X_test)`方法

**注意：** 

- X_train必须是二维矩阵
- y_train如果是标称型（离散型）那就是分类问题Classifier，如果是数值型（连续性）那就是回归问题Regression

我们可以对有限的数据，进行训练集和测试集的拆分：打乱样本集的顺序，进行随机按比例拆分。使用`model_selection.train_test_split()`方法

参数：

- 特征数据和目标数据`(train, target)`：需要打乱的数据
- train_size：float、int，表示训练数据所占的比例
- test_size：float、int，表示测试数据所占的比例
- random_state：int、RandomState instance，随机数种子，控制随机方式，可以控制每次随机的结果是一样的，便于调参
- shuffle：布尔值，是否重组数据，如果是Flase，则stratify也必须为False
- stratify：数组类型或None，按照数组数据分层，作为分类标签

返回四个值：`X_train, X_test, y_train, y_test`用来训练和测试模型


```python
knnclf.fit(X_train, y_train)
# 给训练模型评分，得到训练模型里面的最优解
knnclf.score(X_test, y_test)
```

会得到一个0到1的评分，越大说明模型越好

4. 用数据进行测试

用一组模拟的，或者真实的，知道其结果的数据，进行测试，查看模型的预测分类能力。

```python
knnclf.predict(np.array([17.5, 10.1, 0.5, 7.5, 3.3, 2.2, 5.1]).reshape(1, -1))
```
对得出的结果进行评测。

模型评测的标准：

1. 分类模型
    - 准确率：`model.score(X_test, y_test)`，越大，说明预测正确的越多，算法越好
2. 回归模型：均方误差，`metrics.MSE(y_true, y_predict)`，越大说明误差越大，算法越差




